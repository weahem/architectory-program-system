from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
import time
import os
import re
import requests
from urllib.parse import urljoin, quote
from pathlib import Path

class CyberLeninkaPDFScraper:
    def __init__(self):
        self.base_url = "https://cyberleninka.ru"
        self.download_dir = "downloaded_articles_pdf"
        os.makedirs(self.download_dir, exist_ok=True)
        self.driver = None
        self.setup_driver()
        
    def setup_driver(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Chrome –¥—Ä–∞–π–≤–µ—Ä–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF"""
        chrome_options = Options()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        prefs = {
            "download.default_directory": os.path.abspath(self.download_dir),
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "plugins.always_open_pdf_externally": True,  # –í—Å–µ–≥–¥–∞ –æ—Ç–∫—Ä—ã–≤–∞—Ç—å PDF –≤–Ω–µ—à–Ω–µ
            "safebrowsing.enabled": True
        }
        chrome_options.add_experimental_option("prefs", prefs)
        
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
    def search_and_download_articles(self, query, max_results=12):
        """–ü–æ–∏—Å–∫ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –≤ PDF"""
        print(f"üîç –ü–æ–∏—Å–∫ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
        
        try:
            # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π
            search_url = f"{self.base_url}/search?q={quote(query)}"
            self.driver.get(search_url)
            time.sleep(8)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            self.driver.save_screenshot("search_page.png")
            print("üíæ –°–∫—Ä–∏–Ω—à–æ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
            
            # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏
            article_links = self._find_article_links(max_results)
            print(f"üìé –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏: {len(article_links)}")
            
            if not article_links:
                print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏")
                return 0
            
            # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF —Å—Ç–∞—Ç–µ–π
            downloaded_count = 0
            for i, article_url in enumerate(article_links):
                print(f"üì• –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å—é {i+1}/{len(article_links)}...")
                
                try:
                    success = self._download_article_pdf(article_url, i+1)
                    if success:
                        downloaded_count += 1
                        print(f"‚úÖ PDF —Å—Ç–∞—Ç—å–∏ {i+1} —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω")
                    else:
                        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å PDF —Å—Ç–∞—Ç—å–∏ {i+1}")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏ {i+1}: {e}")
                    continue
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(2)
            
            print(f"üéâ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –£—Å–ø–µ—à–Ω–æ: {downloaded_count}/{len(article_links)}")
            return downloaded_count
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            return 0
    
    def _find_article_links(self, max_results):
        """–ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏"""
        article_links = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
        try:
            # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
            selectors = [
                'a[href*="/article/"]',
                '.search-result a',
                '.article a',
                '.item a',
                '.card a',
                'h2 a',
                'h3 a'
            ]
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        href = element.get_attribute("href")
                        if (href and "/article/" in href and 
                            "search" not in href and 
                            href not in article_links):
                            article_links.append(href)
                            if len(article_links) >= max_results:
                                return article_links
                except:
                    continue
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å—Å—ã–ª–æ–∫: {e}")
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Å—ã–ª–æ–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥
        if len(article_links) < max_results:
            article_links.extend(self._alternative_find_links(max_results - len(article_links)))
        
        return article_links[:max_results]
    
    def _alternative_find_links(self, max_links):
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫"""
        links = []
        try:
            all_anchors = self.driver.find_elements(By.TAG_NAME, "a")
            for anchor in all_anchors:
                href = anchor.get_attribute("href")
                if (href and "/article/" in href and 
                    "search" not in href and 
                    href not in links):
                    links.append(href)
                    if len(links) >= max_links:
                        break
        except:
            pass
        return links
    
    def _download_article_pdf(self, article_url, article_number):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF —Å—Ç–∞—Ç—å–∏"""
        try:
            print(f"   üìÑ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å—Ç–∞—Ç—å–∏: {article_url}")
            self.driver.get(article_url)
            time.sleep(5)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏
            self.driver.save_screenshot(f"article_page_{article_number}.png")
            
            # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏ –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            title = self._get_article_title()
            print(f"   üìù –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏: {title}")
            
            # –ò—â–µ–º –∫–Ω–æ–ø–∫—É/—Å—Å—ã–ª–∫—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF
            pdf_url = self._find_pdf_link()
            
            if pdf_url:
                print(f"   üìé –ù–∞–π–¥–µ–Ω PDF: {pdf_url}")
                return self._download_pdf_file(pdf_url, title, article_number)
            else:
                print(f"   ‚ùå PDF —Å—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã...")
                return self._try_alternative_pdf_download(title, article_number)
                
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ PDF: {e}")
            return False
    
    def _find_pdf_link(self):
        """–ü–æ–∏—Å–∫ —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF"""
        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–Ω–æ–ø–æ–∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF –Ω–∞ CyberLeninka
        pdf_selectors = [
            'a[href*=".pdf"]',
            'a[href*="/pdf/"]',
            'a[href*="download"]',
            '.pdf-download',
            '.download',
            '[class*="pdf"]',
            '[class*="download"]',
            'button[onclick*="pdf"]',
            'button[onclick*="download"]'
        ]
        
        for selector in pdf_selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫—É —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
                    pdf_url = self._get_pdf_url_from_element(element)
                    if pdf_url:
                        return pdf_url
            except:
                continue
        
        return None
    
    def _get_pdf_url_from_element(self, element):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ PDF URL –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            # –°–ø–æ—Å–æ–± 1: –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –∏–∑ href
            href = element.get_attribute("href")
            if href and (".pdf" in href or "/pdf/" in href):
                return href if href.startswith("http") else urljoin(self.base_url, href)
            
            # –°–ø–æ—Å–æ–± 2: –°—Å—ã–ª–∫–∞ –∏–∑ onclick
            onclick = element.get_attribute("onclick")
            if onclick:
                # –ò—â–µ–º URL –≤ onclick
                url_match = re.search(r"['\"](https?://[^'\"]+\.pdf)['\"]", onclick)
                if url_match:
                    return url_match.group(1)
                
                # –ò—â–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏
                rel_match = re.search(r"['\"](/[^'\"]+\.pdf)['\"]", onclick)
                if rel_match:
                    return urljoin(self.base_url, rel_match.group(1))
            
            # –°–ø–æ—Å–æ–± 3: data-–∞—Ç—Ä–∏–±—É—Ç—ã
            data_url = element.get_attribute("data-url") or element.get_attribute("data-href")
            if data_url and (".pdf" in data_url or "/pdf/" in data_url):
                return data_url if data_url.startswith("http") else urljoin(self.base_url, data_url)
                
        except:
            pass
        
        return None
    
    def _try_alternative_pdf_download(self, title, article_number):
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF"""
        try:
            # –ú–µ—Ç–æ–¥ 1: –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—É—Ç—å PDF –Ω–∞ CyberLeninka
            current_url = self.driver.current_url
            if "/article/" in current_url:
                # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—É—Ç—å –∫ PDF
                article_id = current_url.split("/article/")[-1]
                pdf_url = f"{self.base_url}/article/{article_id}.pdf"
                
                print(f"   üîÑ –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π PDF –ø—É—Ç—å: {pdf_url}")
                if self._download_pdf_file(pdf_url, title, article_number):
                    return True
            
            # –ú–µ—Ç–æ–¥ 2: –ò—â–µ–º –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_source = self.driver.page_source
            pdf_matches = re.findall(r'https?://[^"\']+\.pdf', page_source)
            for pdf_url in pdf_matches:
                if "cyberleninka" in pdf_url:
                    print(f"   üîÑ –ù–∞–π–¥–µ–Ω PDF –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ: {pdf_url}")
                    if self._download_pdf_file(pdf_url, title, article_number):
                        return True
            
            # –ú–µ—Ç–æ–¥ 3: –ü—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ API –∏–ª–∏ –¥—Ä—É–≥–∏–µ –ø—É—Ç–∏
            pdf_urls_to_try = [
                current_url.replace("/article/", "/pdf/"),
                current_url + ".pdf",
                current_url + "/download"
            ]
            
            for pdf_url in pdf_urls_to_try:
                print(f"   üîÑ –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π URL: {pdf_url}")
                if self._download_pdf_file(pdf_url, title, article_number):
                    return True
                    
        except Exception as e:
            print(f"   ‚ùå –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏: {e}")
        
        return False
    
    def _download_pdf_file(self, pdf_url, title, article_number):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF —Ñ–∞–π–ª–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            safe_title = self._create_safe_filename(title)
            filename = f"{article_number:02d}_{safe_title}.pdf"
            filepath = os.path.join(self.download_dir, filename)
            
            print(f"   üíæ –°–∫–∞—á–∏–≤–∞–µ–º PDF –≤: {filename}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º requests –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Referer': self.driver.current_url
            }
            
            response = requests.get(pdf_url, headers=headers, stream=True, timeout=30)
            response.raise_for_status()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª —Å–∫–∞—á–∞–Ω –∏ –Ω–µ –ø—É—Å—Ç–æ–π
            if os.path.exists(filepath) and os.path.getsize(filepath) > 1000:
                print(f"   ‚úÖ PDF —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename} ({os.path.getsize(filepath)} –±–∞–π—Ç)")
                return True
            else:
                print(f"   ‚ùå –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π –∏–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω")
                if os.path.exists(filepath):
                    os.remove(filepath)
                return False
                
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF: {e}")
            return False
    
    def _get_article_title(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_selectors = [
                'h1',
                '.article-title',
                '.title',
                'h2',
                '[class*="title"]'
            ]
            
            for selector in title_selectors:
                try:
                    element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    title = element.text.strip()
                    if title and len(title) > 5:
                        return title
                except:
                    continue
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º title —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            return self.driver.title.replace(" - –ö–∏–±–µ—Ä–õ–µ–Ω–∏–Ω–∫–∞", "").strip()
            
        except:
            return f"–°—Ç–∞—Ç—å—è_{int(time.time())}"
    
    def _create_safe_filename(self, title):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        # –£–¥–∞–ª—è–µ–º –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
        # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        safe_title = re.sub(r'\s+', ' ', safe_title).strip()
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        safe_title = safe_title[:100]
        return safe_title
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –¥—Ä–∞–π–≤–µ—Ä–∞"""
        if self.driver:
            self.driver.quit()

# –¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç
def test_pdf_download():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF"""
    print("üöÄ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF —Å—Ç–∞—Ç–µ–π...")
    
    scraper = CyberLeninkaPDFScraper()
    
    try:
        query = "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
        result = scraper.search_and_download_articles(query, 3)
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: —Å–∫–∞—á–∞–Ω–æ {result} PDF —Ñ–∞–π–ª–æ–≤")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–∫–∞—á–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        download_dir = os.path.abspath(scraper.download_dir)
        if os.path.exists(download_dir):
            files = os.listdir(download_dir)
            print(f"üìÅ –§–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ {download_dir}:")
            for file in files:
                filepath = os.path.join(download_dir, file)
                size = os.path.getsize(filepath)
                print(f"   üìÑ {file} ({size} –±–∞–π—Ç)")
                
    finally:
        scraper.close()

if __name__ == "__main__":
    test_pdf_download()